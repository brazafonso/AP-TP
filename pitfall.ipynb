{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3dff691-31c6-406a-90ec-d35ba7025426",
   "metadata": {},
   "source": [
    "# **0. Install/import dependendcies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9dca7-732f-4811-a912-851191e91b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"install libraries\"\"\"\n",
    "!pip install gym gym[atari]\n",
    "!pip install opencv-python\n",
    "!pip install shimmy>=0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545ac9e-cc21-4302-bf86-3d20a877b38d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327345ac-48c9-4b68-8e8a-97a83cf778b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install \"stable-baselines3[extra]>=2.0.0a4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ea2de2-0f2e-48f6-9d8a-2d7e6f3a1899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Import libraries\"\"\"\n",
    "import gymnasium as gym\n",
    "from gym import Env\n",
    "import stable_baselines3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "from matplotlib import animation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22c149c5-fe69-4bef-a934-be1676d26a85",
   "metadata": {},
   "source": [
    "# **1. Starting and exploring Pitfall environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d257ec-d4c5-485a-857f-e3aea8dffeb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Pitfall-v5\", render_mode=\"rgb_array\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ee431-8321-407c-99ee-d777b25c14ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ada74-d637-495b-8da3-a65d2c6f4ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6103a-145b-4e08-a55a-7227bc05fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dimensions of state and action space\n",
    "print(\"State space: {}\".format(env.observation_space))\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Action space: {env.unwrapped.get_action_meanings()}\")\n",
    "\n",
    "# Sample random action\n",
    "action =env.action_space.sample()\n",
    "print(\"Action: {}\".format(action))\n",
    "next_state, reward, done,_,info = env.step(action)\n",
    "\n",
    "# Print output\n",
    "print(\"Reward: {}\".format(reward))\n",
    "print(f\"State done :{done}\")\n",
    "print(f\"State info :{info}\")\n",
    "\n",
    "# Render and plot an environment frame\n",
    "frame = env.render()\n",
    "plt.imshow(frame)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d719e37-7628-46a9-b8ea-8551561dede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_animation(experience_buffer):\n",
    "    \"\"\"Function to run animation\"\"\"\n",
    "    time_lag = 0.05  # Delay (in s) between frames\n",
    "    for experience in experience_buffer:\n",
    "        # Plot frame\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(experience['frame'])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        print(f\"Episode: {experience['episode']}/{experience_buffer[-1]['episode']}\")\n",
    "        print(f\"Epoch: {experience['epoch']}/{experience_buffer[-1]['epoch']}\")\n",
    "        #print(f\"State: {experience['state']}\")\n",
    "        print(f\"Action: {experience['action']}\")\n",
    "        print(f\"Reward: {experience['reward']}\")\n",
    "        # Pauze animation\n",
    "        sleep(time_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdefe3a-ad26-4248-bdb0-6c1bfb62ff5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def store_episode_as_gif(experience_buffer, path='./', filename='animation.gif'):\n",
    "    \"\"\"Store episode as gif animation\"\"\"\n",
    "    fps = 5   # Set framew per seconds\n",
    "    dpi = 300  # Set dots per inch\n",
    "    interval = 50  # Interval between frames (in ms)\n",
    "\n",
    "    # Retrieve frames from experience buffer\n",
    "    frames = []\n",
    "    for experience in experience_buffer:\n",
    "        frames.append(experience['frame'])\n",
    "\n",
    "    # Fix frame size\n",
    "    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Generate animation\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=interval)\n",
    "\n",
    "    # Save output as gif\n",
    "    anim.save(path + filename, writer='imagemagick', fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd9d57-f603-4886-a33f-72632c17140c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Simulation with random agent\"\"\"\n",
    "epoch = 0\n",
    "num_failed_dropoffs = 0\n",
    "experience_buffer = []\n",
    "cum_reward = 0\n",
    "\n",
    "done = False\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "#while not done: ## full run - takes a long time\n",
    "while epoch < 50:\n",
    "    # Sample random action\n",
    "    \"Action selection without action mask\"\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    \"Action selection with action mask\"\n",
    "    #action = env.action_space.sample(env.action_mask(state))\n",
    "\n",
    "    state, reward, done, _,info = env.step(action)\n",
    "    cum_reward += reward\n",
    "\n",
    "    # Store experience in dictionary\n",
    "    experience_buffer.append({\n",
    "        \"frame\": env.render(),\n",
    "        \"episode\": 1,\n",
    "        \"epoch\": epoch,\n",
    "        \"state\": state,\n",
    "        \"action\": action,\n",
    "        \"reward\": cum_reward,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if reward == -10:\n",
    "        num_failed_dropoffs += 1\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "# Run animation and print console output\n",
    "run_animation(experience_buffer)\n",
    "store_episode_as_gif(experience_buffer)\n",
    "\n",
    "print(\"# epochs: {}\".format(epoch))\n",
    "print(\"# failed drop-offs: {}\".format(num_failed_dropoffs))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0f7f5b3-6d60-4987-bed7-1e9c95dae810",
   "metadata": {},
   "source": [
    "# **2. Trying existing models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f019394-794c-40de-b5f1-0361b35002d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
    "from gymnasium.wrappers import GrayScaleObservation\n",
    "from stable_baselines3 import PPO, DQN\n",
    "import torch\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7d556-96b4-4125-a642-52420e5e4ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "device = get_default_device()\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "680bd5ff-b013-480c-b011-8a2a65e01836",
   "metadata": {},
   "source": [
    "## **Proximal Policy Optimization (PPO) - no modifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20ef11b-d20f-4fd8-894e-b679d53dc171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = 'model'\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c2a62-a035-42e3-952a-104f1cffa293",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=20000, save_path='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645385bc-b10f-4057-b9cf-30c3ea0127b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Adapt env to return gray pictures and stack 4 frames\"\"\"\n",
    "env.reset()\n",
    "env_PPO = GrayScaleObservation(env,keep_dim=True)\n",
    "env_PPO = DummyVecEnv([lambda: env_PPO])\n",
    "env_PPO = VecFrameStack(env_PPO, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9dd11f-60cd-4268-b73e-bc2b472489ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578ee25-179b-48da-8336-8031c0e5eb1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Create model\"\"\"\n",
    "model = PPO('CnnPolicy', env_PPO, verbose=1, learning_rate=0.000001, n_steps=512) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e426d2e-b58e-49ee-8a09-9e46ef76dc53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Train Model\"\"\"\n",
    "model.learn(total_timesteps=1000000,callback=callback,log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c85f2-f25e-4264-ad2b-904830c4a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save Model\"\"\"\n",
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb74075-0ff4-400e-835b-b2da7f964595",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load Model\"\"\"\n",
    "model = PPO.load('model.zip',env=env_PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c8308-c2e5-40ab-8271-db81329181ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test Model\"\"\"\n",
    "import time\n",
    "# Start the game \n",
    "state = env_PPO.reset()\n",
    "done = False\n",
    "# Loop through the game\n",
    "while not done: \n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env_PPO.step(action)\n",
    "    frame = env_PPO.render()\n",
    "    clear_output()\n",
    "    plt.imshow(frame)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    time.sleep(0.01)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "442457b6-8b36-4e9f-b80a-5f4e1186553f",
   "metadata": {},
   "source": [
    "## **Proximal Policy Optimization (PPO) - costum wrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c09023-3c3f-457e-bb5d-5a2e7162775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import ResizeObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8f657-2397-4a41-ae19-45c40891ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Wrapper):\n",
    "    \n",
    "    def __init__(self, env): \n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.lives = 3\n",
    "        \n",
    "    def step(self, action):\n",
    "        before_lives = self.lives\n",
    "        next_state, reward, done,truncated, info = self.env.step(action)\n",
    "        lives = info['lives']\n",
    "        # intensifies rewards\n",
    "        if reward < 0 or reward > 0:\n",
    "            reward *= 10\n",
    "            \n",
    "        # losing a life or all lifes is very penalised\n",
    "        if before_lives > lives:\n",
    "            reward -= 100000\n",
    "            done = True\n",
    "        if lives == 0:\n",
    "            reward -= 100000 \n",
    "        self.lives = lives\n",
    "        return next_state, reward, done,truncated, info \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5d730-6691-4eda-a332-c93056aec309",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = CustomEnv(ResizeObservation(env,(130,130)))\n",
    "env_test = GrayScaleObservation(env_test,keep_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e112bd-7a27-427a-8ea7-991fd00c85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c5744-e257-457a-8772-f69ea41cc1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dimensions of state and action space\n",
    "print(\"State space: {}\".format(env_test.observation_space))\n",
    "print(f\"Action space: {env_test.action_space}\")\n",
    "#print(f\"Action space: {env_test.unwrapped.get_action_meanings()}\")\n",
    "\n",
    "# Sample random action\n",
    "action = 3\n",
    "print(\"Action: {}\".format(action))\n",
    "next_state, reward, done,truncated,info = env_test.step(action)\n",
    "\n",
    "# Print output\n",
    "print(\"Reward: {}\".format(reward))\n",
    "print(f\"State done :{done}\")\n",
    "print(f\"State truncated :{truncated}\")\n",
    "print(f\"State info :{info}\")\n",
    "\n",
    "# Render and plot an environment frame\n",
    "#frame = env.render()\n",
    "plt.imshow(next_state)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f725c433-cc4e-46f0-b81b-9f560229fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Action space: ['NOOP', 'FIRE', 'UP', 'RIGHT',\n",
    "#               'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', \n",
    "#               'DOWNRIGHT', 'DOWNLEFT', 'UPFIRE', 'RIGHTFIRE',\n",
    "#               'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE',\n",
    "#               'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n",
    "\n",
    "env_test.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    clear_output()\n",
    "    action = 16\n",
    "    next_state, reward, done,truncated,info = env_test.step(action)\n",
    "    plt.imshow(next_state)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6dbab2-fea9-467d-acb6-95ad64225d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = DummyVecEnv([lambda: env_test])\n",
    "env_test = VecFrameStack(env_test, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e394b-e270-4d97-8d06-89c67e476fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"State space: {}\".format(env_test.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44896ed1-e862-4456-af2e-ac35e724ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done,info = env_test.step([action])\n",
    "plt.figure(figsize=(20,16))\n",
    "for idx in range(next_state.shape[3]):\n",
    "    plt.subplot(1,4,idx+1)\n",
    "    plt.imshow(next_state[0][:,:,idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d80d36-7e77-47c8-bb82-440d941254dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = 'model_custom4'\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ab470-ff3d-4e5c-afbe-3cc047c66cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=100000, save_path='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ee5a4-e99f-49ef-9028-4dcce4b675d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Create model\"\"\"\n",
    "model = PPO('CnnPolicy', env_test, verbose=1, learning_rate=0.000001, n_steps=512) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f5a94-6390-409d-bdca-7ef9745fa7c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=100000,callback=callback,log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a2c62-ad15-4cf2-83e2-aed537ea2b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save Model\"\"\"\n",
    "model.save('model_custom2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f5cec-0912-414d-9486-3181c5e97798",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load Model\"\"\"\n",
    "model = PPO.load('model_custom4.zip',env=env_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d67219-7984-44be-a4db-099c00b55e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test Model\"\"\"\n",
    "\n",
    "# Start the game \n",
    "state = env_test.reset()\n",
    "done = False\n",
    "# Loop through the game\n",
    "while not done: \n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env_test.step(action)\n",
    "    frame = env_test.render()\n",
    "    clear_output()\n",
    "    plt.imshow(frame)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f7b99-d8db-4c8f-8e33-bc7c40edc868",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "num_failed_dropoffs = 0\n",
    "experience_buffer = []\n",
    "cum_reward = 0\n",
    "\n",
    "done = False\n",
    "min_reward = 0\n",
    "state = env_test.reset()\n",
    "\n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(state)\n",
    "\n",
    "    state, reward, done, _ = env_test.step(action)\n",
    "    cum_reward += reward\n",
    "    # Store experience in dictionary\n",
    "    experience_buffer.append({\n",
    "        \"frame\": env_test.render(),\n",
    "        \"episode\": 1,\n",
    "        \"epoch\": epoch,\n",
    "        \"state\": state,\n",
    "        \"action\": action,\n",
    "        \"reward\": cum_reward,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if reward == -10:\n",
    "        num_failed_dropoffs += 1\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "# Run animation and print console output\n",
    "run_animation(experience_buffer)\n",
    "store_episode_as_gif(experience_buffer,\"animation2.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d1a0b6-0141-4f8b-95f2-3d8f2ef7acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [x['action'] for x in experience_buffer]\n",
    "unique_actions = set()\n",
    "for a in actions:\n",
    "    unique_actions.add(a[0])\n",
    "\n",
    "print(unique_actions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e64550c",
   "metadata": {},
   "source": [
    "# **Deep Q Network (DQN) - no modifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c931f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = 'modelDQN'\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d344687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a306753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Adapt env to return gray pictures and stack 4 frames\"\"\"\n",
    "env.reset()\n",
    "env_DQN = GrayScaleObservation(env,keep_dim=True)\n",
    "env_DQN = DummyVecEnv([lambda: env_DQN])\n",
    "env_DQN = VecFrameStack(env_PPO, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0626a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create model\"\"\"\n",
    "model = DQN('CnnPolicy', env_DQN, verbose=1, learning_rate=0.00001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train Model\"\"\"\n",
    "model.learn(total_timesteps=100000,callback=callback,log_interval=10,progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save Model\"\"\"\n",
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb108de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load Model\"\"\"\n",
    "model = PPO.load('model.zip',env=env_DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test Model\"\"\"\n",
    "import time\n",
    "# Start the game \n",
    "state = env_PPO.reset()\n",
    "done = False\n",
    "# Loop through the game\n",
    "while not done: \n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env_PPO.step(action)\n",
    "    frame = env_PPO.render()\n",
    "    clear_output()\n",
    "    plt.imshow(frame)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    time.sleep(0.01)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f41f360b",
   "metadata": {},
   "source": [
    "## **Deep Q Network (DQN) - custom wrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import ResizeObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Wrapper):\n",
    "    \n",
    "    def __init__(self, env): \n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.lives = 3\n",
    "        \n",
    "    def step(self, action):\n",
    "        before_lives = self.lives\n",
    "        next_state, reward, done,truncated, info = self.env.step(action)\n",
    "        lives = info['lives']\n",
    "        # intensifies rewards and truncates to the interval [-1,1]\n",
    "        if reward > 0:\n",
    "            if((reward *2)<1):\n",
    "                reward *= 2\n",
    "            else:\n",
    "                reward = 1\n",
    "        elif reward < 0:\n",
    "            if((reward *2)>-1):\n",
    "                reward *= 2\n",
    "            else:\n",
    "                reward = -1\n",
    "        \n",
    "\n",
    "        # losing a life or all lifes is very penalised\n",
    "        if before_lives > lives:\n",
    "            reward = -1\n",
    "            done = True\n",
    "        if lives == 0:\n",
    "            reward = -1\n",
    "        self.lives = lives\n",
    "        return next_state, reward, done,truncated, info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_custom_DQN = CustomEnv(ResizeObservation(env,(130,130)))\n",
    "env_custom_DQN = GrayScaleObservation(env_custom_DQN,keep_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b8a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_custom_DQN.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d369b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dimensions of state and action space\n",
    "print(\"State space: {}\".format(env_custom_DQN.observation_space))\n",
    "print(f\"Action space: {env_custom_DQN.action_space}\")\n",
    "#print(f\"Action space: {env_custom_DQN.unwrapped.get_action_meanings()}\")\n",
    "\n",
    "# Sample random action\n",
    "action = 3\n",
    "print(\"Action: {}\".format(action))\n",
    "next_state, reward, done,truncated,info = env_custom_DQN.step(action)\n",
    "\n",
    "# Print output\n",
    "print(\"Reward: {}\".format(reward))\n",
    "print(f\"State done :{done}\")\n",
    "print(f\"State truncated :{truncated}\")\n",
    "print(f\"State info :{info}\")\n",
    "\n",
    "# Render and plot an environment frame\n",
    "#frame = env.render()\n",
    "plt.imshow(next_state)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_custom_DQN.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    clear_output()\n",
    "    action = 4\n",
    "    next_state, reward, done,truncated,info = env_custom_DQN.step(action)\n",
    "    plt.imshow(next_state)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe8d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_custom_DQN = DummyVecEnv([lambda: env_custom_DQN])\n",
    "env_custom_DQN = VecFrameStack(env_custom_DQN, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"State space: {}\".format(env_test.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done,info = env_custom_DQN.step([action])\n",
    "plt.figure(figsize=(20,16))\n",
    "for idx in range(next_state.shape[3]):\n",
    "    plt.subplot(1,4,idx+1)\n",
    "    plt.imshow(next_state[0][:,:,idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdad757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = 'model_custom_DQN'\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01955bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce087b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create model\"\"\"\n",
    "model = DQN('CnnPolicy', env_custom_DQN, verbose=1, learning_rate=0.00001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0239c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=10000,callback=callback,log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save Model\"\"\"\n",
    "model.save('model_custom_DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c11115",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load Model\"\"\"\n",
    "model = DQN.load('model_custom_DQN',env=env_custom_DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test Model\"\"\"\n",
    "\n",
    "# Start the game \n",
    "state = env_test.reset()\n",
    "done = False\n",
    "# Loop through the game\n",
    "while not done: \n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env_test.step(action)\n",
    "    frame = env_test.render()\n",
    "    clear_output()\n",
    "    plt.imshow(frame)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7251c907-bef5-441e-8265-988ced324dd0",
   "metadata": {},
   "source": [
    "# **3. Making costum model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0f359-c21c-4820-becd-f176318a48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d3cb68-f07f-438a-8b43-ec47898f31c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce20890-7d21-4735-912e-d47d8037b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa0855-601e-4ed6-ba65-e1b844ddf75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_c = env\n",
    "#env_c = GrayScaleObservation(env_c,keep_dim=True)\n",
    "height, width, channels = env_c.observation_space.shape\n",
    "actions = env_c.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db367e-27d5-445c-9a68-df3e4eb2cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_c = CustomEnv(env_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7110c72-e94e-4e91-bae5-9f49d369c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done,_,info = env_c.step(action)\n",
    "\n",
    "# Print output\n",
    "print(f\"Next state: {next_state.shape}\")\n",
    "print(f\"State info :{info}\")\n",
    "print(f\"State done :{done}\")\n",
    "print(\"Reward: {}\".format(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c37d9-d9ac-4ef7-b7f5-5a5d5d76e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131dc9cc-1aca-485c-85c8-74373efea099",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a590ee9-9c0d-4603-97dc-f0db895accec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f007c-4975-4470-953f-e0fad8c727dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0f343-7c87-4312-92c2-bbfd07e0120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-4), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35967e-3d6f-42b7-9db5-c133236cde55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.fit(env_c, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3611480-9253-4ee5-b3fe-c525fde6a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn.test(env_c, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
