{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 1. Install Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-07T15:29:27.894268Z","iopub.status.busy":"2023-06-07T15:29:27.893010Z"},"trusted":true},"outputs":[],"source":["%pip install gymnasium\n","%pip install 'gymnasium[atari]'\n","%pip install 'gymnasium[accept-rom-license]'\n","%pip install 'shimmy>=0.2.1'\n","%pip install torch torchvision torchaudio\n","%pip install matplotlib\n","%pip install tensorboard\n","%pip install 'stable-baselines3[extra]>=2.0.0a4'\n","%pip install scikit-image"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 2. Import Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import gymnasium\n","from gymnasium import spaces\n","from gymnasium.spaces import Box\n","\n","import matplotlib.pyplot as plt\n","from matplotlib import animation\n","from IPython.display import clear_output\n","\n","import time\n","import numpy as np\n","from skimage.transform import resize\n","\n","import torch\n","from torch import Tensor\n","from torch.nn import Linear\n","from torch.nn import Conv2d\n","from torch.nn import MaxPool2d\n","from torch.nn import ReLU\n","from torch.nn import Module\n","\n","from stable_baselines3.common.callbacks import BaseCallback\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from stable_baselines3 import DQN\n","from stable_baselines3.common.evaluation import evaluate_policy\n","from stable_baselines3.common.torch_layers import BaseFeaturesExtractor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Change to True if you want to run on Kaggle\n","# Also, the underground model should be uploaded to Kaggle\n","KAGGLE = False"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 3. Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class UndergroundModel(Module):\n","    def __init__(self):\n","        super(UndergroundModel, self).__init__()\n","        self.conv1 = Conv2d(3, 16, kernel_size=10, stride=1, padding=2)\n","        self.conv2 = Conv2d(16, 32, kernel_size=10, stride=1, padding=1)\n","        self.conv3 = Conv2d(32, 64, kernel_size=7, stride=1, padding=1)\n","        self.fc1 = Linear(7744, 16)\n","        self.fc2 = Linear(16, 16)\n","        self.maxpool = MaxPool2d(kernel_size=2, stride=2)\n","        self.relu = ReLU()\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = self.conv2(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = self.conv3(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def run_animation(experience_buffer):\n","    \"\"\"Function to run animation\"\"\"\n","    time_lag = 0.05  # Delay (in s) between frames\n","    for experience in experience_buffer:\n","        # Plot frame\n","        clear_output(wait=True)\n","        plt.imshow(experience['frame'])\n","        plt.axis('off')\n","        plt.show()\n","\n","        \n","        print(f\"Episode: {experience['episode']}/{experience_buffer[-1]['episode']}\")\n","        print(f\"Epoch: {experience['epoch']}/{experience_buffer[-1]['epoch']}\")\n","        print(f\"Action: {experience['action']}\")\n","        print(f\"Reward: {experience['reward']}\")\n","        print(f\"Comulative Reward: {experience['com_reward']}\")\n","        \n","        # Pause animation\n","        time.sleep(time_lag)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def store_episode_as_gif(experience_buffer, filepath='animation.gif'):\n","    \"\"\"Store episode as gif animation\"\"\"\n","    fps = 5   # Set frames per second\n","    dpi = 300  # Set dots per inch\n","    interval = 50  # Interval between frames (in ms)\n","\n","    # Retrieve frames from experience buffer\n","    frames = []\n","    for experience in experience_buffer:\n","        frames.append(experience['frame'])\n","\n","    # Fix frame size\n","    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","\n","    # Generate animation\n","    def animate(i):\n","        patch.set_data(frames[i])\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=interval)\n","\n","    # Save output as gif\n","    anim.save(filepath, writer='imagemagick', fps=fps)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 4. Create Base Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["base_env = gymnasium.make('ALE/Pitfall-ram-v5', render_mode='rgb_array')\n","if base_env.reset():\n","    print('Environment is ready!')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def show_env_info(env):\n","    # Print dimensions of state and action space\n","    print(\"State space: {}\".format(base_env.observation_space))\n","    print(f\"Action space: {env.action_space}\")\n","    print(f\"Action space: {env.unwrapped.get_action_meanings()}\")\n","\n","    # Sample random action\n","    action =env.action_space.sample()\n","    print(\"Action: {}\".format(action))\n","    next_state, reward, done, _, info = env.step(action)\n","\n","    # Print output\n","    print(\"Reward: {}\".format(reward))\n","    print(f\"State done :{done}\")\n","    print(f\"State info :{info}\")\n","\n","    # Render and plot an environment frame\n","    frame = env.render()\n","    plt.imshow(frame)\n","    plt.axis(\"off\")\n","    plt.show()\n","\n","show_env_info(base_env)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 5. Customize Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_path = 'models/model.h5'\n","if KAGGLE:\n","    model_path = '/kaggle/input/' + model_path\n","\n","underground_model = UndergroundModel()\n","underground_model.load_state_dict(torch.load(model_path))\n","underground_model.eval()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 5.1 Normalized Rewards without bonus for left or right"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Reconstructing the action space"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Available actions: 'NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE'\n","\n","Redundant actions: 'UPRIGHT', 'UPLEFT', 'UPFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'DOWNRIGHTFIRE', 'UPLEFTFIRE', 'DOWNLEFTFIRE'\n","\n","'RIGHT' = 'UPRIGHT' \\\n","'LEFT' = 'UPLEFT' \\\n","'FIRE' = 'UPFIRE', 'DOWNFIRE' \\\n","'RIGHTFIRE' = 'UPRIGHTFIRE', 'DOWNRIGHTFIRE' \\\n","'LEFTFIRE' = 'UPLEFTFIRE', 'DOWNLEFTFIRE'  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# The reward values will be between [-1,1]\n","\n","class CustomEnv2(gymnasium.Wrapper):\n","\n","    def __init__(self, env, model):\n","        super(CustomEnv2, self).__init__(env)\n","        self.env = env\n","        self.model = model\n","        self.observation_space = Box(shape=(130,130,3), low=int(0), high=int(255))\n","        self.reset_values()\n","        \n","        \n","    def reset_values(self):\n","        '''Coloca os valores default do ambiente'''\n","        self.lives = 3\n","        self.last_room = None\n","        self.is_zero_before = False\n","        self.n_zeros_before = 0\n","        self.latest_visited_room = 196 # id of first room\n","        self.steps_in_visited_rooms = 0\n","        self.visited_rooms = [196]\n","        self.env.action_space = gymnasium.spaces.Discrete(10)\n","        \n","    def calculate_mse(self, img1, img2):\n","        np_image1 = np.array(img1)\n","        np_image2 = np.array(img2)\n","        mse = np.mean((np_image1 - np_image2) ** 2)\n","        return mse\n","    \n","    def map_actions(self,action):\n","        match action:\n","            case 6:\n","                action = 11\n","            case 7:\n","                action = 12\n","            case _:\n","                pass\n","        return action\n","\n","    def step(self, action):\n","        action = self.map_actions(action)\n","        next_state_ram, reward, done, truncated, info = self.env.step(action)\n","        self.lives = info['lives']\n","\n","        # Normalizing the reward for every treasure to be worth the same\n","        if(reward>0):\n","            if(reward>2000):\n","                reward = 2000\n","            reward = reward/2000\n","\n","        # Normalizing the biggest score penalty\n","        elif(reward<0):\n","            if(reward<-100):\n","                reward = -100\n","            reward = reward/100    \n","\n","        next_state_img = self.env.render()\n","        next_state_img = resize(next_state_img,(130,130))\n","        underground_prediction = self.make_underground_prediction(next_state_img)\n","        # Give negative reward if agent is underground\n","        if underground_prediction == 1:\n","            reward -= 0.25\n","        \n","\n","        current_room_id = next_state_ram[1]\n","\n","        # Penalize agent for staying in the same room to much time (~150 steps)\n","        if current_room_id in self.visited_rooms:\n","            self.steps_in_visited_rooms += 1\n","            if self.steps_in_visited_rooms > 200:\n","                reward -= 0.10\n","                self.steps_in_visited_rooms = 150\n","        else:\n","            self.steps_in_visited_rooms = 0\n","        \n","        # Reward if agent discovers a new room\n","        if current_room_id not in self.visited_rooms:\n","            reward += 0.75\n","            self.visited_rooms.append(current_room_id)\n","            \n","        self.latest_visited_room = current_room_id\n","\n","        # Giving negative reward to death\n","        # Try to detect death ASAP\n","        mse_score = None\n","        if self.last_room is not None:\n","            mse_score = self.calculate_mse(self.last_room, next_state_img)\n","            if mse_score == 0:\n","                if self.is_zero_before and self.n_zeros_before > 5:\n","                    reward -= 1\n","                    self.n_zeros_before = -9999\n","                \n","                self.is_zero_before = True\n","                self.n_zeros_before += 1\n","                \n","            else:\n","                self.is_zero_before = False\n","                self.n_zeros_before = 0\n","\n","        # Capping maximum positive reward to 1\n","        if(reward>1):\n","            reward = 1\n","\n","        # Capping minimum negative reward to -1\n","        elif(reward<-1):\n","            reward = -1\n","        \n","        self.last_room = next_state_img\n","        return next_state_img, reward, done, truncated, info\n","\n","    def reset(self,seed=None):\n","        if seed:\n","            _,info = self.env.reset(seed=seed)\n","        else:\n","            _,info = self.env.reset()\n","        img = self.env.render()\n","        img = resize(img,(130,130))\n","        self.reset_values()\n","        return (img,info)\n","        \n","\n","    def make_underground_prediction(self, img):\n","        '''Checks if player is underground'''\n","        img = self.preprocessar(img)\n","        img = img.astype(np.float32)\n","        img = Tensor(img)\n","        img = img.reshape(1,3,130,130)\n","        pred = self.model(img)\n","        pred = pred.detach().cpu().detach().numpy()[0].argmax()\n","        return pred\n","\n","    def preprocessar(self,imagem):\n","        imagem = np.array(imagem)\n","        data_mean = np.mean(imagem)\n","        data_std = np.std(imagem)\n","        imagem = (imagem - data_mean) / data_std\n","        xmax, xmin = imagem.max(), imagem.min()\n","        imagem = (imagem - xmin)/(xmax - xmin)\n","        imagem = imagem.transpose(2,1,0)\n","        return imagem\n","\n","env = CustomEnv2(base_env, underground_model)\n","if env.reset():\n","    print('Custom Environment is ready!')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 6. Custom Model: Define and Train"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Print action space and number of actions\n","n_actions = env.action_space.n\n","actions = env.unwrapped.get_action_meanings()\n","print(f\"Number of actions: {n_actions}, Actions: {actions}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CustomCNN(BaseFeaturesExtractor):\n","    def __init__(self, observation_space: spaces.Box, features_dim: int = 256):\n","        super().__init__(observation_space, features_dim)\n","        n_input_channels = observation_space.shape[0]\n","        \n","        self.cnn = torch.nn.Sequential(\n","            torch.nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n","            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n","            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n","            torch.nn.Flatten(),\n","        )\n","        \n","\n","        # Compute shape by doing one forward pass\n","        with torch.no_grad():\n","            n_flatten = self.cnn(\n","                torch.as_tensor(observation_space.sample()[None]).float()\n","            ).shape[1]\n","\n","\n","        self.linear = torch.nn.Sequential(\n","            torch.nn.Linear(n_flatten, features_dim),\n","            torch.nn.ReLU()\n","        )\n","\n","    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n","        return self.linear(self.cnn(observations))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["policy_kwargs = dict(\n","    features_extractor_class=CustomCNN,\n","    features_extractor_kwargs=dict(features_dim=128),\n",")\n","# The buffer_size parameter equals 25000 needs around 10GB of Grafic Memory  \n","model = DQN(policy=\"CnnPolicy\", env=env, buffer_size=25000, policy_kwargs=policy_kwargs, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class TensorboardCallback(BaseCallback):\n","    def __init__(self, log_dir, save_freq, save_path, verbose=0):\n","        super(TensorboardCallback, self).__init__(verbose)\n","        self.writer = SummaryWriter(log_dir=log_dir)\n","        self.save_freq = save_freq\n","        self.save_path = save_path\n","\n","    def _on_step(self) -> bool:\n","        # Log training metrics\n","        self.writer.add_scalar(\"Reward\", self.locals[\"rewards\"][0], self.num_timesteps)\n","\n","        # Save model every `self.save_freq` steps\n","        if self.num_timesteps % self.save_freq == 0:\n","            self.model.save(self.save_path+f\"_{self.num_timesteps}\")\n","\n","        return True\n","\n","    def _on_training_end(self) -> None:\n","        # Close the SummaryWriter after training\n","        self.writer.close()\n","\n","# Specify the log directory where TensorBoard files will be saved\n","log_dir = \"logs/\"\n","\n","# Create a TensorboardCallback\n","tensorboard_callback = TensorboardCallback(log_dir=log_dir, save_freq=25000, save_path='(kaggle) ppo_pitfall_1M')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train the agent\n","model.learn(total_timesteps=1000000, callback=tensorboard_callback)\n","\n","# Save the model\n","model.save(\"(kaggle) dqn_pitfall_1M_final\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 7. Test Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Loading the model\n","model = DQN.load(\"models_kaggle_DQN_norm_rewards_w_b_l_or_r_diff_r_f_s_r/(kaggle) dqn_pitfall_1M_final\")\n","\n","experience_buffer = []\n","obs = env.reset()[0]\n","com_reward = 0\n","terminated = False\n","epoch = 0\n","\n","start_time = time.time()\n","\n","while epoch <= 1:\n","    epoch += 1\n","    while not terminated:\n","        action, _ = model.predict(obs)\n","        next_obs, reward, terminated, truncated, info = env.step(action)\n","        com_reward += reward\n","\n","        experience_buffer.append({\n","            'episode': info['episode_frame_number'],\n","            'epoch': epoch,\n","            'frame': env.render(),\n","            'action': action,\n","            'reward': reward,\n","            'state': obs,\n","            'com_reward': com_reward,\n","        })\n","        \n","        obs = next_obs\n","        \n","        if time.time() - start_time > 30:\n","            break\n","\n","run_animation(experience_buffer)\n","store_episode_as_gif(experience_buffer, filepath='animation.gif')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
