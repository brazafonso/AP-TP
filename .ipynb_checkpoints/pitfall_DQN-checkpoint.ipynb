{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium\n",
    "%pip install 'gymnasium[atari]'\n",
    "%pip install 'gymnasium[accept-rom-license]'\n",
    "%pip install 'shimmy>=0.2.1'\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install matplotlib\n",
    "%pip install tensorboard\n",
    "%pip install 'stable-baselines3[extra]>=2.0.0a4'\n",
    "%pip install scikit-image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Module\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to True if you want to run on Kaggle\n",
    "# Also, the underground model should be uploaded to Kaggle\n",
    "KAGGLE = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de previsão se o jogador está debaixo de terra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UndergroundModel(Module):\n",
    "    def __init__(self):\n",
    "        super(UndergroundModel, self).__init__()\n",
    "        self.conv1 = Conv2d(3, 16, kernel_size=10, stride=1, padding=2)\n",
    "        self.conv2 = Conv2d(16, 32, kernel_size=10, stride=1, padding=1)\n",
    "        self.conv3 = Conv2d(32, 64, kernel_size=7, stride=1, padding=1)\n",
    "        self.fc1 = Linear(7744, 16)\n",
    "        self.fc2 = Linear(16, 16)\n",
    "        self.maxpool = MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para correr e criar gif de uma execução do ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_animation(experience_buffer):\n",
    "    \"\"\"Function to run animation\"\"\"\n",
    "    time_lag = 0.05  # Delay (in s) between frames\n",
    "    for experience in experience_buffer:\n",
    "        # Plot frame\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(experience['frame'])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        print(f\"Episode: {experience['episode']}/{experience_buffer[-1]['episode']}\")\n",
    "        print(f\"Epoch: {experience['epoch']}/{experience_buffer[-1]['epoch']}\")\n",
    "        print(f\"Action: {experience['action']}\")\n",
    "        print(f\"Reward: {experience['reward']}\")\n",
    "        print(f\"Comulative Reward: {experience['com_reward']}\")\n",
    "        \n",
    "        # Pause animation\n",
    "        time.sleep(time_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_episode_as_gif(experience_buffer, filepath='animation.gif'):\n",
    "    \"\"\"Store episode as gif animation\"\"\"\n",
    "    fps = 5   # Set frames per second\n",
    "    dpi = 300  # Set dots per inch\n",
    "    interval = 50  # Interval between frames (in ms)\n",
    "\n",
    "    # Retrieve frames from experience buffer\n",
    "    frames = []\n",
    "    for experience in experience_buffer:\n",
    "        frames.append(experience['frame'])\n",
    "\n",
    "    # Fix frame size\n",
    "    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Generate animation\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=interval)\n",
    "\n",
    "    # Save output as gif\n",
    "    anim.save(filepath, writer='imagemagick', fps=fps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create Base Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiente base do Pitfall!, versão de observações de RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = gymnasium.make('ALE/Pitfall-ram-v5', render_mode='rgb_array')\n",
    "if base_env.reset():\n",
    "    print('Environment is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_env_info(env):\n",
    "    # Print dimensions of state and action space\n",
    "    print(\"State space: {}\".format(base_env.observation_space))\n",
    "    print(f\"Action space: {env.action_space}\")\n",
    "    print(f\"Action space: {env.unwrapped.get_action_meanings()}\")\n",
    "\n",
    "    # Sample random action\n",
    "    action =env.action_space.sample()\n",
    "    print(\"Action: {}\".format(action))\n",
    "    next_state, reward, done, _, info = env.step(action)\n",
    "\n",
    "    # Print output\n",
    "    print(\"Reward: {}\".format(reward))\n",
    "    print(f\"State done :{done}\")\n",
    "    print(f\"State info :{info}\")\n",
    "\n",
    "    # Render and plot an environment frame\n",
    "    frame = env.render()\n",
    "    plt.imshow(frame)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_env_info(base_env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Customize Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar modelo treinado UndergroundModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/model.h5'\n",
    "if KAGGLE:\n",
    "    model_path = '/kaggle/input/' + model_path\n",
    "\n",
    "underground_model = UndergroundModel()\n",
    "underground_model.load_state_dict(torch.load(model_path))\n",
    "underground_model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Ambiente do Modelo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características :  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- normalização dos rewards\n",
    "- observação fornecida pelo ambiente é a frame atual (ao contrário dos valores da RAM do ambiente original), resized (130x130) \n",
    "- mais rápida verificação de morte\n",
    "    - verificação de frames seguidas semelhantes (algo que apenas acontece diversas vezas consecutivas no momento de morte)\n",
    "- reward por entrar em salas não visitadas\n",
    "    - utilizando informação da RAM \n",
    "- penalização por permanecer debaixo de terra\n",
    "    - envolve resizing da imagem (130x130) para passagem para o UndergroundModel\n",
    "- penalização por permanecer demasiados steps na mesma sala\n",
    "- penalização por morte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing the action space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available actions: 'NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE'\n",
    "\n",
    "Redundant actions: 'UPRIGHT', 'UPLEFT', 'UPFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'DOWNRIGHTFIRE', 'UPLEFTFIRE', 'DOWNLEFTFIRE'\n",
    "\n",
    "'RIGHT' = 'UPRIGHT' \\\n",
    "'LEFT' = 'UPLEFT' \\\n",
    "'FIRE' = 'UPFIRE', 'DOWNFIRE' \\\n",
    "'RIGHTFIRE' = 'UPRIGHTFIRE', 'DOWNRIGHTFIRE' \\\n",
    "'LEFTFIRE' = 'UPLEFTFIRE', 'DOWNLEFTFIRE'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gymnasium.Wrapper):\n",
    "\n",
    "    def __init__(self, env, model):\n",
    "        super(CustomEnv, self).__init__(env)\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.observation_space = Box(shape=(130,130,3), low=int(0), high=int(255)) # change observation space\n",
    "        self.reset_values()\n",
    "        self.env.action_space = gymnasium.spaces.Discrete(10)  # limit action space\n",
    "        \n",
    "        \n",
    "    def reset_values(self):\n",
    "        '''Reset environment values'''\n",
    "        self.lives = 3\n",
    "        self.last_room_img = None # last visited room\n",
    "        self.is_zero_before = False # difference between last two frames\n",
    "        self.n_zeros_before = 0 # number of consecutive similar frames\n",
    "        self.latest_visited_room = 196 # id of first room\n",
    "        self.steps_in_visited_rooms = 0\n",
    "        self.visited_rooms = [196] # list of visited rooms\n",
    "        \n",
    "    def calculate_mse(self, img1, img2):\n",
    "        '''Calculates similarity between images'''\n",
    "        np_image1 = np.array(img1)\n",
    "        np_image2 = np.array(img2)\n",
    "        mse = np.mean((np_image1 - np_image2) ** 2)\n",
    "        return mse\n",
    "    \n",
    "    def map_actions(self,action):\n",
    "        '''Remaps certain actions'''\n",
    "        match action:\n",
    "            case 6:\n",
    "                action = 11\n",
    "            case 7:\n",
    "                action = 12\n",
    "            case _:\n",
    "                pass\n",
    "        return action\n",
    "\n",
    "    def step(self, action):\n",
    "        '''Override step method'''\n",
    "        action = self.map_actions(action)\n",
    "        next_state_ram, reward, done, truncated, info = self.env.step(action)\n",
    "        # Get number of lives left\n",
    "        self.lives = info['lives']\n",
    "        # Get current room\n",
    "        current_room_id = next_state_ram[1]\n",
    "        \n",
    "        # Normalizing the reward for every treasure to be worth the same\n",
    "        if(reward>0):\n",
    "            if(reward>2000):\n",
    "                reward = 2000\n",
    "            reward = reward/2000\n",
    "\n",
    "        # Normalizing the biggest score penalty\n",
    "        elif(reward<0):\n",
    "            if(reward<-100):\n",
    "                reward = -100\n",
    "            reward = reward/100    \n",
    "            \n",
    "        # Get current frame\n",
    "        next_state_img = self.env.render()\n",
    "        # Resize frame\n",
    "        next_state_img = resize(next_state_img,(130,130))\n",
    "\n",
    "        # Predict if player is underground\n",
    "        underground_prediction = self.make_underground_prediction(next_state_img)\n",
    "        # Give negative reward if agent is underground\n",
    "        if underground_prediction == 1:\n",
    "            reward -= 0.25\n",
    "        \n",
    "        # Penalize agent for staying in the same room too much time (~150 steps)\n",
    "        if current_room_id in self.visited_rooms:\n",
    "            self.steps_in_visited_rooms += 1\n",
    "            if self.steps_in_visited_rooms > 200:\n",
    "                reward -= 0.15\n",
    "                self.steps_in_visited_rooms = 150\n",
    "        else:\n",
    "            self.steps_in_visited_rooms = 0\n",
    "        \n",
    "        # Reward if agent discovers a new room\n",
    "        if current_room_id not in self.visited_rooms:\n",
    "            reward += 0.25\n",
    "            self.visited_rooms.append(current_room_id)          \n",
    "\n",
    "        # Update last visited room\n",
    "        self.latest_visited_room = current_room_id\n",
    "\n",
    "        # Giving negative reward for death\n",
    "        # Try to detect death ASAP\n",
    "        mse_score = None\n",
    "        if self.last_room_img is not None:\n",
    "            mse_score = self.calculate_mse(self.last_room_img, next_state_img)\n",
    "            if mse_score == 0:\n",
    "                # five identical frames corresponds to death\n",
    "                if self.is_zero_before and self.n_zeros_before > 5:\n",
    "                    reward -= 1\n",
    "                    self.n_zeros_before = -9999\n",
    "                \n",
    "                self.is_zero_before = True\n",
    "                self.n_zeros_before += 1\n",
    "                \n",
    "            else:\n",
    "                self.is_zero_before = False\n",
    "                self.n_zeros_before = 0\n",
    "\n",
    "        # Normalizing rewards\n",
    "        if(reward>1):\n",
    "            reward = 1\n",
    "        elif(reward<-1):\n",
    "            reward = -1\n",
    "\n",
    "        # Updating last room frame\n",
    "        self.last_room_img = next_state_img\n",
    "        return next_state_img, reward, done, truncated, info\n",
    "\n",
    "    def reset(self,seed=None):\n",
    "        '''Override the reset method'''\n",
    "        if seed:\n",
    "            _,info = self.env.reset(seed=seed)\n",
    "        else:\n",
    "            _,info = self.env.reset()\n",
    "        # render needs to return a frame observation\n",
    "        img = self.env.render()\n",
    "        img = resize(img,(130,130))\n",
    "        self.reset_values()\n",
    "        return (img,info)\n",
    "        \n",
    "\n",
    "    def make_underground_prediction(self, img):\n",
    "        '''Checks if player is underground'''\n",
    "        img = self.preprocessar(img)\n",
    "        img = img.astype(np.float32)\n",
    "        img = Tensor(img)\n",
    "        img = img.reshape(1,3,130,130)\n",
    "        pred = self.model(img)\n",
    "        pred = pred.detach().cpu().detach().numpy()[0].argmax()\n",
    "        return pred\n",
    "\n",
    "    def preprocessar(self,imagem):\n",
    "        '''Process frame for the underground model'''\n",
    "        imagem = np.array(imagem)\n",
    "        data_mean = np.mean(imagem)\n",
    "        data_std = np.std(imagem)\n",
    "        imagem = (imagem - data_mean) / data_std\n",
    "        xmax, xmin = imagem.max(), imagem.min()\n",
    "        imagem = (imagem - xmin)/(xmax - xmin)\n",
    "        imagem = imagem.transpose(2,1,0)\n",
    "        return imagem\n",
    "\n",
    "# Wrap env\n",
    "env = CustomEnv(base_env, underground_model)\n",
    "if env.reset():\n",
    "    print('Custom Environment is ready!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Custom Model: Define and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de extração de features do algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        \n",
    "        self.cnn = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            torch.nn.Flatten(),\n",
    "        )\n",
    "        \n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "\n",
    "\n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_flatten, features_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo de treino - DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    ")\n",
    "# The buffer_size parameter equals 25000 needs around 10GB of Grafic Memory  \n",
    "model = DQN(policy=\"CnnPolicy\", env=env, buffer_size=25000, policy_kwargs=policy_kwargs, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback para guardar resultados do treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardCallback(BaseCallback):\n",
    "    def __init__(self, log_dir, save_freq, save_path, verbose=0):\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "        self.writer = SummaryWriter(log_dir=log_dir)\n",
    "        self.save_freq = save_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Log training metrics\n",
    "        self.writer.add_scalar(\"Reward\", self.locals[\"rewards\"][0], self.num_timesteps)\n",
    "\n",
    "        # Save model every `self.save_freq` steps\n",
    "        if self.num_timesteps % self.save_freq == 0:\n",
    "            self.model.save(self.save_path+f\"_{self.num_timesteps}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # Close the SummaryWriter after training\n",
    "        self.writer.close()\n",
    "\n",
    "# Specify the log directory where TensorBoard files will be saved\n",
    "log_dir = \"logs/\"\n",
    "\n",
    "# Create a TensorboardCallback\n",
    "tensorboard_callback = TensorboardCallback(log_dir=log_dir, save_freq=25000, save_path='(kaggle) dqn_pitfall_1M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treino do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "model.learn(total_timesteps=1000000, callback=tensorboard_callback)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"(kaggle) dqn_pitfall_1M_final\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testar modelo:\n",
    "- carregar modelo e ambiente\n",
    "- modelo toma ação de acordo com o estado\n",
    "- toma ações até jogo terminar\n",
    "- correr e criar animação\n",
    "- para correr os modelos, é pode ser usado apenas o ambiente base com observações humanas e com um wrapper que da resize do frame para 130x130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "model=DQN.load(\"models/model2/model2_final.zip\")\n",
    "\n",
    "experience_buffer = []\n",
    "obs = env.reset()[0]\n",
    "com_reward = 0\n",
    "terminated = False\n",
    "start_time = time.time()\n",
    "epoch = 0\n",
    "# run while not finished, or at most 30 seconds\n",
    "while not terminated:\n",
    "    action, _ = model.predict(obs)\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    com_reward += reward\n",
    "\n",
    "    experience_buffer.append({\n",
    "        'episode': info['episode_frame_number'],\n",
    "        'epoch': epoch,\n",
    "        'frame': env.render(),\n",
    "        'action': action,\n",
    "        'reward': reward,\n",
    "        'state': obs,\n",
    "        'com_reward': com_reward,\n",
    "    })\n",
    "    \n",
    "    obs = next_obs\n",
    "    \n",
    "    if time.time() - start_time > 30:\n",
    "        break\n",
    "\n",
    "run_animation(experience_buffer)\n",
    "store_episode_as_gif(experience_buffer, filepath='animation.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
